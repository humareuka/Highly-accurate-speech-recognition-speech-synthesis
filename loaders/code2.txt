import os
import time
from pydub import AudioSegment
from pydub.silence import split_on_silence
import onse_gose2
import pyaudio
import wave
from pykakasi import kakasi
import whisper
import logging
import warnings

print("起動準備中です...")
warnings.filterwarnings("ignore")
os.system("title RVCP v2.1.6")
import requests
import json

data_models = ["tiny", "base", "small", "medium", "large"]
url = 'http://localhost:50031/'
query_p = 'audio_query'
req_url = url + query_p
enable_interrogative_upspeak = ''

v_name = "test.mp3"
v_dir = "./"
voice_pass = v_dir + v_name

# APIに送信する情報
# speaker_idには（重要）しゃべらせたいボイスのstyleIdを書いてください（上記はつくよみちゃんれいせいです）　
my_text = 'これはテストです。'

headers = {'speaker': 1}
moji = ""
for tmp in "RVCP Version 2.1.6":
    os.system("cls")
    moji += tmp
    print(moji)
    if not tmp == " ":
        time.sleep(0.1)
time.sleep(3)
os.system("cls")


def sleep(t):
    time.sleep(t)


# print(json.dumps(json_data))
# GETリクエストを送る
def Get_Request():
    response = requests.get(
        'http://localhost:50031/core_versions',
    )
    res_data = response.json()
    print(res_data)
    print(response.status_code)


# POSTリクエストを送る（クエリを作成）
def Post_RequestMQ(text, speaker_id):
    q_params = {'text': text, 'speaker': speaker_id, 'core_version': '0.0.0'}
    response = requests.post(req_url, params=q_params)
    res_data = response.json()
    # print(res_data)
    if not str(response.status_code) == "200":
        print("処理に失敗")
    return res_data


# POSTリクエストを送る(音声合成し、返ってきた音声ファイルを保存)
def Post_RequestMA(f_path, my_query, speaker_id):
    a_params = {'speaker': speaker_id, 'core_version': '0.0.0', 'enable_interrogative_upspeak': 'true'}
    response = requests.post('http://localhost:50031/synthesis', params=a_params, data=json.dumps(my_query))
    with open(f"./音声/{f_path}.wav", 'wb') as saved_voice:
        saved_voice.write(response.content)
    if not str(response.status_code) == "200":
        print("処理に失敗")


# ログレベルを DEBUG に変更
logging.basicConfig(filename='log.log', level=logging.DEBUG)
logging.critical('critical')
logging.error('error')
logging.warning('warning')
logging.info('info')
logging.debug('debug')

kakasi = kakasi()
# モードの設定：J(Kanji) to H(Hiragana)
kakasi.setMode('J', 'H')


def play_wav(wav_file_path):
    CHUNK = 1024

    # pyaudioの初期化
    p = pyaudio.PyAudio()

    # WAVファイルを開く
    with wave.open(wav_file_path, 'rb') as wav_file:
        stream = p.open(format=p.get_format_from_width(wav_file.getsampwidth()),
                        channels=wav_file.getnchannels(),
                        rate=wav_file.getframerate(),
                        output=True)

        # チャンクごとに音声を再生
        data = wav_file.readframes(CHUNK)
        while data:
            stream.write(data)
            data = wav_file.readframes(CHUNK)

        # 再生終了
        stream.stop_stream()
        stream.close()

    # pyaudioを終了
    p.terminate()
print("起動準備完了")
time.sleep(1)
os.system("cls")

# モデル（データセット）読み込み
while True:
    count_tmp = 0
    for tmp in data_models:
        count_tmp += 1
        print(str(count_tmp), ":", tmp)
    voice_num = input("使用したいモデルを数字で指定してください : ")
    tmp = int(voice_num) - 1
    print(data_models[tmp], "のモデルでよろしいですか？")
    check = input("Y / N : ")
    if check == "Y" or check == "y":
        break
print("モデルの読み込みを開始します")
model = whisper.load_model(data_models[tmp])
print("モデルの読み込み完了")
time.sleep(3)
os.system("cls")
ch = True
while ch:
    print("COEIROINKの起動を確認中・・・")
    koe_lists = []
    try:
        r = requests.get("http://localhost:50031/speakers").json()
        ch = False
    except Exception:
        os.system("cls")
        print("起動していないか、\nAPIにアクセスできないようです。")
        sleep(3)
        print("COEIROINKの状態を確認してから再度起動してみてください")
        sleep(1)
        os.system("cls")
print("起動を確認しました")
while True:
    count_tmp = 0
    for tmp in r:
        for tmp2 in tmp['styles']:
            count_tmp += 1
            t_name = tmp['name'] + " - " + tmp2['name']
            print(str(count_tmp), ":", t_name, tmp['name'])
            koe_lists.append([count_tmp, tmp2['id'], tmp['name']])
    voice_num = input("使用したい声を数字で指定してください : ")
    tmp = int(voice_num) - 1
    print(koe_lists[tmp][2], "の音声でよろしいですか？")
    check = input("Y / N : ")
    if check == "Y" or check == "y":
        break
print("声を", koe_lists[tmp][2], "にしました")
sleep(1)
os.system("cls")
voice_id = koe_lists[tmp][1]
f_name = "test"

CHUNK = 2 ** 10
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 44100
record_time = 15
output_path = "output.wav"

while True:
    print("録音中・・・")
    import pyaudio
    import wave
    import numpy as np
    from datetime import datetime
    import time

    # 音データフォーマット
    chunk = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    audio = pyaudio.PyAudio()
    RATE = 44100

    threshold = 0.1  # 閾値
    start = time.time()  # 計測開始

    while True:
        # 音の取込開始
        stream = audio.open(format=FORMAT,
                            channels=CHANNELS,
                            rate=RATE,
                            input=True,
                            frames_per_buffer=chunk
                            )
        # 音データの取得
        data = stream.read(chunk)
        # ndarrayに変換
        x = np.frombuffer(data, dtype="int16") / 32768.0

        if x.max() > threshold:
            end = time.time()  # 計測終了
            finish = False
            frames = []
            # 録音処理
            while True:
                for i in range(0, int(RATE / chunk * 1)):
                    data = stream.read(chunk)
                    frames.append(data)
                    ndarray = np.frombuffer(data, dtype="int16") / 32768.0
                    print("閾値＝" + str(ndarray.max()))
                if ndarray.max() < 0.3:
                    zr = 0
                    while True:
                        data = stream.read(chunk)
                        frames.append(data)
                        ndarray = np.frombuffer(data, dtype="int16") / 32768.0
                        if zr >= 1:
                            finish = True
                            break
                        if ndarray.max() > threshold:
                            zr = 0
                            print(zr)
                            break
                        sleep(0.01)
                        zr += 0.01
                        print(zr)
                if finish:
                    break
            break

    # 録音終了処理
    stream.stop_stream()
    stream.close()
    audio.terminate()

    # 録音データをファイルに保存
    wav = wave.open("output.wav", 'wb')
    wav.setnchannels(CHANNELS)
    wav.setsampwidth(audio.get_sample_size(FORMAT))
    wav.setframerate(RATE)
    wav.writeframes(b''.join(frames))
    wav.close()
    print("録音完了")
    print("無音部分を削除します")

    sound = AudioSegment.from_file("output.wav", "wav")

    chunks = split_on_silence(sound,
                              min_silence_len=100,
                              silence_thresh=-35,
                              keep_silence=100)

    cutted_sound = sum(chunks)
    cutted_sound.export('output.wav')
    print("完了")
    print("音声を文字に変換中・・・")

    # 音声へのパス
    path = "output.wav"

    # 結果を出力と同時に取得
    result = model.transcribe(path, language='ja', fp16=False)
    print("認識完了")
    text = result["text"]
    print("認識結果 :", text)

    print("音声合成中・・・")

    # 変換して出力
    print("ひらがなに変換中・・・")
    conv = kakasi.getConverter()
    text = conv.do(text)
    print("変換結果：", text)

    tmp = Post_RequestMQ(text, voice_id)
    Post_RequestMA(f_name, tmp, voice_id)
    print("音声合成完了")

    print("再生中・・・")
    play_wav(f"./音声/{f_name}.wav")
    os.remove("./output.wav")
    print("再生完了")
